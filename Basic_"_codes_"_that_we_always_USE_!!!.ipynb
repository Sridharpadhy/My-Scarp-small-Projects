{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUybnBudRXfdx5Yx7kBhrS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sridharpadhy/My-Scarp-small-Projects/blob/main/Basic_%22_codes_%22_that_we_always_USE_!!!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1."
      ],
      "metadata": {
        "id": "BhfxyVARzE-N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hMuIqInyueg"
      },
      "outputs": [],
      "source": [
        "#importing necessary libraries \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import pylab\n",
        "from scipy import stats\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from scipy.stats import zscore\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "import math\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "!pip install scikit-optimize\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn import ensemble\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2."
      ],
      "metadata": {
        "id": "4HluNIYuzC-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lSLF7ypSzDoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3."
      ],
      "metadata": {
        "id": "Npx_NaQezO0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - Creation ( bar chart )\n",
        "plt.subplots(figsize=(15,8))\n",
        "sns.barplot(data= dataframe_name ,x= 'independent variable',y='dependent variable',capsize=.2 ).set(title = 'title name')"
      ],
      "metadata": {
        "id": "EqlEE259zPnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4."
      ],
      "metadata": {
        "id": "HGM7jW0dz4Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing displots to analyze the distribution of all numerical features\n",
        "# plot a bar plot for each numerical feature count \n",
        "#gca - get current axis . \n",
        "for col in numeric_feat[u can right ratio u wana cut down or use inside this ]:\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = dataframe_name[col]\n",
        "    #feature = np.log1p(final_df[col]) u can also use logp here only to reduce skewness.\n",
        "    sns.distplot(feature,ax = ax)    \n",
        "    ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mpfjNjJOz5LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5."
      ],
      "metadata": {
        "id": "Rw9wc-fO0bDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation is the corr shows between both independent and dependent variable.\n",
        "# \" Now this z and y_hat is the code used to create the red line in the scatter plot which is the best fit line\n",
        "# z is showing the polynomial fit for numpy array like this is showing 1-d array.\n",
        "# The line of best fit estimates a straight line that minimizes the distance between itself and where observations fall in some data set. \n",
        "# The line of best fit is used to show a trend or correlation between the dependent variable and independent variable(s).\n",
        "# Also shows us it is positive or negative correlation.\n",
        "\n",
        "for col in numeric_feat:\n",
        "  if col not in [here name of all column dont want to see correlation with ]:\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = final_df[col]      (dataframe name the [col])\n",
        "    label = final_df['Sales']     (dependent variable column)\n",
        "    correlation = feature.corr(label)\n",
        "    plt.scatter(x=feature, y=label)\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Sales')\n",
        "    ax.set_title('Sales vs ' + col + '- correlation: ' + str(correlation))\n",
        "    z = np.polyfit(final_df[col], final_df['Sales'], 1)\n",
        "    y_hat = np.poly1d(z)(final_df[col])\n",
        "\n",
        "    plt.plot(final_df[col], y_hat, \"r--\", lw=1)\n",
        "\n",
        "    \n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pV-jpWBB0boF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6."
      ],
      "metadata": {
        "id": "Z2FPnUWi1nYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking multicollinearity with the vif function.\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "GF_Qq3jB1oTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(final_df[[i for i in final_df.describe().columns if i not in ['name of the column u wana drop ']]])\n",
        "#final_df = dataframe name ."
      ],
      "metadata": {
        "id": "QpfoZreF2JKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7."
      ],
      "metadata": {
        "id": "8Q4gsA7N2IgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data of independent variables\n",
        "X = final_df[indep_var].values\n",
        "\n",
        "# Create the dependent variable data\n",
        "y = final_df[dep_var].values"
      ],
      "metadata": {
        "id": "btoD82NY2Zjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
        "#here 0.2 means 80-20 split."
      ],
      "metadata": {
        "id": "hlXm2Kmv2hoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8."
      ],
      "metadata": {
        "id": "jCDhuRYZ2qh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming data( This is  done so that all values are between 0 and 1 .)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "WktyCwgs2rap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9."
      ],
      "metadata": {
        "id": "Bhro0LM42vyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to train the input model and print evaluation matrix and evaluation graphs\n",
        "def analyse_model(model, X_train, X_test, y_train, y_test):\n",
        "\n",
        "  '''Takes regressor model and train test splits as input and prints the\n",
        "  evaluation matrices with the plot and returns the model'''\n",
        "\n",
        "  # Fitting the model\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred_train=model.predict(X_train)\n",
        "  y_pred_test=model.predict(X_test)\n",
        "\n",
        "  # Calculating Evaluation Matrix\n",
        "  mse_train = mean_squared_error(y_train,y_pred_train)\n",
        "  mse_test = mean_squared_error(y_test,y_pred_test)\n",
        "  rmse_train = np.sqrt(mse_train)\n",
        "  rmse_test = np.sqrt(mse_test)\n",
        "  r2_score_train = r2_score(y_train,y_pred_train)\n",
        "  r2_score_test=r2_score(y_test,y_pred_test)\n",
        "  try:\n",
        "    importance = model.feature_importances_\n",
        "    feature = feature\n",
        "  except:\n",
        "    importance = np.abs(model.coef_)\n",
        "    feature = indep_var\n",
        "  indices = np.argsort(importance)\n",
        "  indices = indices[::-1]\n",
        "\n",
        "  # Printing Evaluation Matrix\n",
        "  print(\"MSE of train dataset:\" , mse_train)\n",
        "  print(\"MSE of test dataset:\" , mse_test)\n",
        "  print(\"RMSE of train dataset :\" ,rmse_train)\n",
        "  print(\"RMSE of test dataset :\" ,rmse_test)\n",
        "  print(\"MAE of train dataset:\" ,mean_absolute_error(y_train,y_pred_train))\n",
        "  print(\"MAE of test dataset:\" ,mean_absolute_error(y_test,y_pred_test))\n",
        "  print(\"Train R2 :\", r2_score_train)\n",
        "  print(\"Test R2 :\" ,r2_score_test)\n",
        "  print(\"Adjusted R2 : train dataset\", 1-(1-r2_score_train)*((len(X_train)-1)/(len(X_train)-X_train.shape[1]-1)))\n",
        "  print(\"Adjusted R2 of test dataset: \", 1-(1-r2_score_test)*((len(X_test)-1)/(len(X_test)-X_test.shape[1]-1)))\n",
        "\n",
        "  # Plotting actual and predicted values and the feature importances:\n",
        "  plt.figure(figsize=(18,9))\n",
        "  plt.subplot(3,1,1)\n",
        "  plt.plot((y_pred_test)[:100])\n",
        "  plt.plot((np.array(y_test)[:100]))\n",
        "  plt.legend([\"Predicted\",\"Actual\"])\n",
        "  plt.title('Actual and Predicted sales')\n",
        "  plt.subplot(3,1,2)\n",
        "  plt.bar(range(len(indices)),importance[indices])\n",
        "  plt.xticks(range(len(indices)), [feature[i] for i in indices])\n",
        "  plt.title('Feature Importance')\n",
        "  plt.subplot(3,1,3)\n",
        "  plt.scatter((y_pred_test),(y_test),c= 'purple')\n",
        "  plt.title('Hetroscadacity')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "AFNTSI7w2weL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10."
      ],
      "metadata": {
        "id": "PzQjfY6b2_j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some model doesnt have function of .coef for that we will use this fn code."
      ],
      "metadata": {
        "id": "1_z8DjuX3BQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to train the input model and print evaluation matrix\n",
        "def analyse_decision (model, X_train, X_test, y_train, y_test):\n",
        "\n",
        "  '''Takes regressor model and train test splits as input and prints the\n",
        "  evaluation matrices with the plot and returns the model'''\n",
        "\n",
        "  # Fitting the model\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred_train=model.predict(X_train)\n",
        "  y_pred_test=model.predict(X_test)\n",
        "\n",
        "  # Calculating Evaluation Matrix\n",
        "  mse_train = mean_squared_error(y_train,y_pred_train)\n",
        "  mse_test = mean_squared_error(y_test,y_pred_test)\n",
        "  rmse_train = np.sqrt(mse_train)\n",
        "  rmse_test = np.sqrt(mse_test)\n",
        "  r2_score_train = r2_score(y_train,y_pred_train)\n",
        "  r2_score_test=r2_score(y_test,y_pred_test)\n",
        "  try:\n",
        "    importance = model.feature_importances_\n",
        "    feature = feature\n",
        "  except:\n",
        "    feature = indep_var\n",
        "  indices = np.argsort(importance)\n",
        "  indices = indices[::-1]\n",
        "\n",
        "  # Printing Evaluation Matrix\n",
        "  print(\"MSE of train dataset:\" , mse_train)\n",
        "  print(\"MSE of test dataset:\" , mse_test)\n",
        "  print(\"RMSE of train dataset :\" ,rmse_train)\n",
        "  print(\"RMSE of test dataset :\" ,rmse_test)\n",
        "  print(\"MAE of train dataset:\" ,mean_absolute_error(y_train,y_pred_train))\n",
        "  print(\"MAE of test dataset:\" ,mean_absolute_error(y_test,y_pred_test))\n",
        "  print(\"Train R2 :\", r2_score_train)\n",
        "  print(\"Test R2 :\" ,r2_score_test)\n",
        "  print(\"Adjusted R2 : train dataset\", 1-(1-r2_score_train)*((len(X_train)-1)/(len(X_train)-X_train.shape[1]-1)))\n",
        "  print(\"Adjusted R2 of test dataset: \", 1-(1-r2_score_test)*((len(X_test)-1)/(len(X_test)-X_test.shape[1]-1)))\n",
        "\n",
        "  # Plotting actual and predicted values and the feature importances:\n",
        "  plt.figure(figsize=(18,9))\n",
        "  plt.subplot(3,1,1)\n",
        "  plt.plot((y_pred_test)[:100])\n",
        "  plt.plot((np.array(y_test)[:100]))\n",
        "  plt.legend([\"Predicted\",\"Actual\"])\n",
        "  plt.title('Actual and Predicted sales')\n",
        "  plt.subplot(3,1,2)\n",
        "  plt.bar(range(len(indices)),importance[indices])\n",
        "  plt.xticks(range(len(indices)), [feature[i] for i in indices])\n",
        "  plt.title('Feature Importance')\n",
        "  plt.subplot(3,1,3)\n",
        "  plt.scatter((y_pred_test),(y_test),c= 'purple')\n",
        "  plt.title('Hetroscadacity')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return model\n",
        "  "
      ],
      "metadata": {
        "id": "0EAd2F4F3KQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11."
      ],
      "metadata": {
        "id": "Mp0Uj6a33V60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-10,1e-5,1e-3,1e-1,1,5,10,30,60,100]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "lasso_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "g_iqVe8t3Whk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "steVhN543s0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we can put a best para and create new data file then analyse it by the fn or by directly using \".best_estimator_ \"  \n"
      ],
      "metadata": {
        "id": "pdugMLEN383z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyse_model(lasso_regressor.best_estimator_ , X_train , X_test , y_train, y_test)"
      ],
      "metadata": {
        "id": "hAZEtWa34J3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}